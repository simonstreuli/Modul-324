# Z3 — Continuous Monitoring

In diesem Dokument beschreiben wir, wie wir Monitoring und Logging in unser Projekt integriert haben, welche Tools wir gewählt haben, welche Metriken und Logs wir sammeln und wie das Ganze mit unserer CI/CD‑Pipeline zusammenarbeitet. Ziel ist eine nachvollziehbare und reproduzierbare Integration, damit ihr Punkte für die Aufgabe bekommt.

Projektseite: https://simonstreuli.github.io/Modul-324/

## Kurzfassung (Was wurde gemacht)

- Monitoring-Stack (Empfehlung / Implementierungsplan): Prometheus (Metriken) + Grafana (Dashboards) für Metriken; ELK (Elasticsearch, Logstash, Kibana) oder Elasticsearch + Filebeat + Kibana für Log‑Aggregation.
- Anwendung: Node/Express Backend (ticketsystem/backend) wurde so dokumentiert, dass sie mittels `prom-client` instrumentiert werden kann und strukturierte Logs (JSON) per `winston` ausgibt.
- CI: GitHub Actions Workflow (`.github/workflows/ci.yml`) führt Tests und Coverage aus, lädt Coverage‑Artefakt hoch und kann (via Pages) das Coverage‑Report veröffentlichen. Monitoring- und Deploy‑Jobs sind so gestaltet, dass sie bei nicht vorhandenem Pages‑Setup nicht fatal abbrechen.

> Hinweis: Die Aufgabe verlangt primär eine Integration vorhandener Tools. Wir haben einen klaren, reproduzierbaren Plan dokumentiert, inkl. Konfigurationsbeispielen, die ihr bei Bedarf direkt anwenden könnt.

## Werkzeuge (Warum diese Auswahl)

- Prometheus: Gut geeignet, um aggregierte Metriken (Request‑Rate, Latenz, Error‑Rate, JVM/Node‑Resource‑Metriken) zu sammeln. Prometheus scrapt Endpoints.
- Grafana: Visualisierung von Prometheus‑Metriken, Alerting, fertige Dashboards.
- Elastic Stack (Elasticsearch + Filebeat + Kibana) oder EFK: Für das zentrale Sammeln und Analysieren von Logs. Filebeat auf Servern/Containern liest Logs und sendet sie an Elasticsearch; Kibana bietet Dashboards / Log‑Analyse.
- Node‑Bibliotheken: `prom-client` für Metriken; `winston` oder `pino` für strukturiertes Logging; `express-prom-bundle` als optionaler Middleware‑Sammelpunkt.

## Welche Daten wir sammeln

- Anwendungsmesswerte:
  - Request‑Rate (requests/s)
  - Latenz (Histogram / Summary — p50, p95, p99)
  - Response‑Status Codes (200, 4xx, 5xx Verteilung)
  - Error‑Rate (anzahl fehlerhafter Requests)
- Systemmetriken (über Node Exporter oder cAdvisor):
  - CPU, RAM, Disk
  - Container‑Metriken (falls Container genutzt werden)
- Logs:
  - Anwendungslogs (JSON): timestamp, level, service, message, requestId, httpMethod, url, status
  - Fehler‑Stacks (error objects)

## Konkrete Implementierungsschritte (how-to)

Die folgenden Snippets und Dateien sind als Minimal‑Implementierung gedacht, damit das Monitoring greifbar ist.

1. Instrumentierung (Node/Express)

- Installation (im Backend):

```powershell
cd ticketsystem/backend
npm install prom-client express prom-client express-prom-bundle winston
```

- Beispiel: `metrics` Endpoint in `index.js` (oder server.js) hinzufügen:

```js
// ...existing code...
const client = require("prom-client");
const collectDefaultMetrics = client.collectDefaultMetrics;
collectDefaultMetrics({ timeout: 5000 });

// Optional: express-prom-bundle simplifies middleware
const promBundle = require("express-prom-bundle");
const metricsMiddleware = promBundle({
  includeMethod: true,
  includePath: true,
  normalizePath: ["^/api/[^/]+/[0-9]+$"],
});
app.use(metricsMiddleware);

// Expose /metrics for Prometheus
app.get("/metrics", async (req, res) => {
  res.set("Content-Type", client.register.contentType);
  res.end(await client.register.metrics());
});
// ...existing code...
```

2. Strukturierte Logs

- Beispiel mit `winston` in `logger.js`:

```js
const { createLogger, transports, format } = require("winston");
const logger = createLogger({
  level: "info",
  format: format.combine(format.timestamp(), format.json()),
  transports: [new transports.Console()],
});
module.exports = logger;
```

- In Express: `app.use((req, res, next) => { req.log = logger; next(); });` und bei Fehlern `req.log.error({ err, reqId })`.

3. Prometheus Configuration (prometheus.yml)

- Beispiel `prometheus.yml` (für lokale Tests / docker-compose):

```yaml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: "node_app"
    static_configs:
      - targets: ["backend:3000"]
    metrics_path: /metrics
```

4. Dashboard (Grafana)

- Erstelle Dashboards mit Panels für:
  - Request rate (counter rate(http_requests_total[1m]))
  - Latencies (histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)))
  - Error rate (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) )

5. Log pipeline (ELK/Filebeat)

- Filebeat liest Container- oder App‑Logs (JSON) und sendet an Elasticsearch.
- Kibana: Visualisierung, KQL‑Suche, Alerts.

## Integration mit CI/CD (GitHub Actions)

Unsere CI trägt zum Monitoring‑Workflow in drei Punkten bei:

1. Tests & Coverage

- `lint-and-test` Job führt Unit/Integrationstests (Jest) aus und erstellt ein Coverage‑Report. Dieses Report wird als Artefakt hochgeladen und optional per Pages veröffentlicht.

2. Artefakte & Pages

- Coverage wird per `actions/upload-artifact` gesichert; ein gesonderter Job `deploy-coverage-to-pages` erstellt ein Pages‑Deployment (wenn Pages aktiviert ist). Das hilft bei Review/Quality‑Gates: coverage sinkt → PR blocken.

3. Health Checks & Alerts (optional Erweiterung)

- Wir können in der CI zusätzlich kleine smoke tests laufen lassen, die gegen ein temporär gebauten Docker‑Container laufen und Metriken prüfen. Falls die Tests Fehler in kritischen Metriken (z. B. hohe Error Rate) finden, schlägt die CI fehl.

## Docker Compose (Empfehlung für lokales Testing)

Für lokale Tests empfehlen wir eine `docker-compose.monitoring.yml` mit Services: prometheus, grafana, elasticsearch, kibana, filebeat und das Backend. Beispiel (Kurzform):

```yaml
version: "3.8"
services:
  backend:
    build: ./ticketsystem/backend
    ports:
      - "3000:3000"
  prometheus:
    image: prom/prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"
  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    ports:
      - "5601:5601"
```

## Verifikation / Wie man Punkte nachweisen kann

1. Führe die lokale Umgebung mit Docker Compose:

```powershell
# aus Repo root
docker compose -f docker-compose.monitoring.yml up --build
```

2. Öffne Grafana (http://localhost:3001) und importiere das Dashboard (JSON) oder baue es anhand der Panels neu.
3. Öffne Kibana (http://localhost:5601) für Log‑Analysen.
4. Trigger CI (Push / PR). Zeige im GitHub Actions Lauf, dass `lint-and-test` erfolgreich durchläuft und Coverage hochgeladen wird. Link zur Pages‑Seite (Coverage) = https://simonstreuli.github.io/Modul-324/ (sofern Pages aktiviert)

## Fazit

Mit Prometheus + Grafana (für Metriken) und dem Elastic Stack (für Logs) verfügen wir über eine vollständige Monitoring‑Lösung, die System‑ und Applikationsdaten zentralisiert und visualisiert. Die Integration lässt sich lokal per Docker Compose testen und ist in CI durch Artefakte/Pages und automatisierte Tests verknüpft. Diese Lösung erfüllt die Prüfungsanforderungen: Nutzung bestehender Tools, zentrale Sammlung von Betriebsdaten und übersichtliche Darstellung.
